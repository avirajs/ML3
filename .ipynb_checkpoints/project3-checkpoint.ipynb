{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Preparation and Overview (30 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [20 points] Explain the task and what business-case or use-case it is designed to solve (or designed to investigate). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "\n",
    "This data set is from The Department of Transportation’s Bureau of Transportation Statistics regarding the On Time Performance of domestic flights flying from the DFW airport from January to March of this year. Since the volume of the original data is so large, we decided to only look at flights from DFW to ORD (Chicago O'Hare) for our model. \n",
    "\n",
    "When booking flights for a trip, there are often many different airlines and times of day to choose from. Our classification task is to predict the departure delay group (how long the flight is delayed divided into 14 groups of 15 minute intervals ranging from <-15 minutes (flight is more than 15 minutes early) to >= 180 minutes) for each flight based on the Month, Day of Flight, Day of the Week, Departure Time, Airline. The end goal of analysis on our dataset is to be able to understand for an origin and a destination, in our case DFW to ORD, when is the best time to fly and with what airline in order to minimize departure delays when booking a trip. If our model is successful, it could be trained with other origin and destination cities.\n",
    "\n",
    "## Who would benefit\n",
    "\n",
    "According to air travel intelligence company, OAG, from April 2017 to March 2018 DFW to ORD was DFW’s second highest grossing route, bringing in $358.4 million in revenue. Thus, it is in the airlines best interest to minimize departure delay and maximize customer satisfaction on this route. Although small delays are inevitable, airlines that fly from DFW could use this analysis to help them see how they stack up to their competitors and to help them better schedule their employees in order to account for probable delays. For example, during times when there is a high likelihood of a long delay, they could have more gate and travel agent staff available.\n",
    "\n",
    "\n",
    "Additionally, businesses who send employees on business trips and people taking personal trips flying from DFW to ORD would benefit from this analysis. If a flight for a business trip gets delayed or cancelled, the company loses money as hours of the client or employee's time are wasted as a result. Additionally, many people get a very limited time for vacation and personal trips. A long delay or cancellation can cause them to lose valuable time at their destination or with their families. Using this analysis a person or company can try to schedule their flights to minimize likelihood of experiencing delays when flying from DFW. If a person has to fly during a time or with an airline with a likelihood of a long delay, they can use this analysis to help them plan accordingly and be sure to build in flexibility due to delays when planning their schedules.\n",
    "\n",
    "## Model Performance\n",
    "In order to be considered a useful model, the model will need to be able to accurately predict 90% of the entire dataset. Our model will not be perfect because delay time can be affected by a myriad of factors such as mechanical factors, weather related factors, crew related factors, and the fact that a plane does not just magically appear in an airport but travels from a different destination where it could have also been affected by delays. With the given data our model is unable to account for all of these factors.\n",
    "\n",
    "Our model would most likely be deployed for internal analysis by the airlines and used for offline analysis by  businesses, and customers planning a trip.\n",
    "\n",
    "For businesses and people using this model to try and plan their schedules when taking a personal or business trip a number of false negatives (where a flight is predicted to have a large delay but is not delayed) would not be too much of an issue, but a large number of false positives (where a flight is predicted to be early or have little or no delay but is delayed) would be problematic. Although some delays are unavoidable, it is not ethical or useful to the customer to have a large number of false positives. Because of this, our model would have to have a relative rate of around 1% of false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] (mostly the same processes as from previous labs) Define and prepare your class variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline\n",
    "import missingno as mn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('DallasToChicago.csv') # read in the csv file\n",
    "print('Pandas:', pd.__version__)\n",
    "print('Numpy:',np.__version__)\n",
    "\n",
    "#Remove attributes that are not useful for us\n",
    "for col in ['TailNum','FlightNum','OriginAirportID',\n",
    "           'OriginCityName','OriginState','OriginStateName','DestAirportID','DestCityName','DestState','DestStateName','CRSDepTime',\n",
    "           'DepDelayMinutes','TaxiIn','TaxiOut','CRSArrTime','ArrTime','ArrDelay','ArrDelayMinutes','ArrDelayGroup','ATimeBlk','CancellationReason',\n",
    "            'Diverted', 'AirTime','CarrierDelay','WeatherDelay','NASDelay','SecurityDelay','LateAircraftDelay','FirstDepTime1','FirstDepTime2',\n",
    "            'FirstDepTime','TotalAddGTime','LongestAddGTime','DivAirportLandings','DivReachedDest','DivActualElapsedTime','DivArrDelay','DivDistance',\n",
    "           'CRSElapsedTime','Flights','Cancelled','Unnamed: 0', 'Distance', 'DistGroup', 'ActualElapsedTime']:\n",
    "    if col in df:\n",
    "        del df[col]\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info(verbose=True, null_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure out what data is missing\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn.matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the missing data\n",
    "df.dropna(inplace=True)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).\n",
    "Initial Dataset:\n",
    "* month (the day of the week of the flight) -----------------------> ordinal\n",
    "* dayofmonth (the day of the week of the flight) -----------------------> ordinal\n",
    "* dayofweek (the day of the week of the flight) -----------------------> ordinal\n",
    "* airline (the airline of the flight)--------------------------------> nominal\n",
    "* Origin (origin airport code of the flight)-------------------------> nominal\n",
    "* Dest (destination airport code of the flight)----------------------> nominal\n",
    "* DepTime (departure time of the flight)-----------------------------> ratio\n",
    "* DepDelay (delay of the flight departure in minutes)----------------> interval\n",
    "* DepDelayGroup (delay of the flight departure grouped by minutes)---> ordinal\n",
    "* DTimeBlk (delay of the flight grouped by hours)--------------------> ordinal\n",
    "* ActualElapsedTime (flight time in the air)-------------------------> ratio\n",
    "* Distance (distance the flight travelled)---------------------------> interval\n",
    "* DistGroup (distance the flight travelled grouped by miles----------> ordinal\n",
    "\n",
    "Final Dataset: \n",
    "* month (the day of the week of the flight) -----------------------> int\n",
    "* dayofmonth (the day of the week of the flight) -----------------------> int\n",
    "* dayofweek (the day of the week of the flight) -----------------------> int\n",
    "* airline (the airline of the flight)--------------------------------> one-hot encoded\n",
    "* DepTime (departure time of the flight in seconds)-----------------------------> float\n",
    "* DepDelayGroup (delay of the flight departure grouped by minutes)---> int\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see explained variance\n",
    "import numpy as np\n",
    "\n",
    "def plot_explained_variance(pca):\n",
    "    import plotly\n",
    "    from plotly.graph_objs import Bar, Line\n",
    "    from plotly.graph_objs import Scatter, Layout\n",
    "    from plotly.graph_objs.scatter import Marker\n",
    "    from plotly.graph_objs.layout import XAxis, YAxis\n",
    "    plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "\n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "    cum_var_exp = np.cumsum(explained_var)\n",
    "\n",
    "    plotly.offline.iplot({\n",
    "        \"data\": [Bar(y=explained_var, name='individual explained variance'),\n",
    "                 Scatter(y=cum_var_exp, name='cumulative explained variance')\n",
    "            ],\n",
    "        \"layout\": Layout(xaxis=XAxis(title='Principal components'), yaxis=YAxis(title='Explained variance ratio'))\n",
    "    })\n",
    "\n",
    "features = ['Year','Quarter','Month' ,'DayofMonth','DayOfWeek','DepDelay']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separating out the features\n",
    "x = df.loc[:, features].values\n",
    "# Standardizing the features\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=6)\n",
    "X_pca = pca.fit(x)\n",
    "plot_explained_variance(pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [5 points] Divide you data into training and testing data using an 80% training and 20% testing split. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cross validation modules that are part of scikit-learn. Argue \"for\" or \"against\" splitting your data using an 80/20 split. That is, why is the 80/20 split appropriate (or not) for your dataset?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "def time_converter(t):\n",
    "    x = time.strptime(t.split(',')[0],'%H:%M')\n",
    "    return int(datetime.timedelta(hours=x.tm_hour,minutes=x.tm_min,seconds=x.tm_sec).total_seconds())\n",
    "\n",
    "\n",
    "\n",
    "df[\"RealDepTime\"] = df.apply( lambda row: time_converter(row.DepTime), axis=1)\n",
    "\n",
    "\n",
    "cleaned_df = df[['Month' ,'airline','DayofMonth','DayOfWeek', 'DepDelayGroup','RealDepTime']]\n",
    "columns = cleaned_df # Declare the columns names\n",
    "y = columns\n",
    "\n",
    "#one hot encode airline\n",
    "temp_df = pd.get_dummies(cleaned_df.airline,prefix='airline')\n",
    "cleaned_df = pd.concat((cleaned_df,temp_df),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'airline' in cleaned_df:\n",
    "    del cleaned_df['airline'] # get rid of the original category as it is now one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "number = LabelEncoder()\n",
    "cleaned_df['DepDelayGroupA'] = number.fit_transform(cleaned_df['DepDelayGroup'].astype('str'))\n",
    "#cleaned_df.head()\n",
    "\n",
    "print(cleaned_df['DepDelayGroupA'].nunique())\n",
    "print(df['DepDelayGroup'].nunique())\n",
    "\n",
    "#show the value that coorsponds to original groups\n",
    "comp_df = cleaned_df[['DepDelayGroup' ,'DepDelayGroupA']]\n",
    "print(comp_df.drop_duplicates())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'DepDelayGroup' in cleaned_df:\n",
    "    del cleaned_df['DepDelayGroup'] # get rid of the original category as it is now one-hot encoded\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing vars\n",
    "##y_train, y_test, X_train, X_test = train_test_split(cleaned_df, y, test_size=0.2)\n",
    "#remove label from training dataset\n",
    "#X_train = X_train.drop(['DepDelayGroup'], axis=1)\n",
    "#print(X_train.shape)\n",
    "#print(X_test.shape)\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'DepDelayGroupA' in cleaned_df:\n",
    "    y = cleaned_df['DepDelayGroupA'].values # get the labels we want\n",
    "    del cleaned_df['DepDelayGroupA'] # get rid of the class label\n",
    "    norm_features = ['Month','DayofMonth','DayOfWeek','RealDepTime' ]\n",
    "    cleaned_df[norm_features] = (cleaned_df[norm_features]-cleaned_df[norm_features].mean()) / cleaned_df[norm_features].std()\n",
    "    X = cleaned_df.values # use everything else to predict!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cv_iterations = 1\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(\n",
    "                         n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "print( cv_object.split(X,y))\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "\n",
    "X_train, y_train, X_test, y_test = ([] for i in range(4))\n",
    "\n",
    "for train_indices, test_indices in cv_object.split(X,y):\n",
    "    # I will create new variables here so that it is more obvious what\n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "num_cv_iterations = 1\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(\n",
    "                         n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "print( cv_object.split(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = ([] for i in range(4))\n",
    "\n",
    "for train_indices, test_indices in cv_object.split(X,y):\n",
    "    # I will create new variables here so that it is more obvious what\n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explaniaiton of training split\n",
    "# divide into testing and training\n",
    "# a ration of 80:20 would be great for our dataset because it is neither too small nor too bigself.\n",
    "# an average dataset like this one would not need more than 20% of data as testing because 362 is already enough to capture most of the variation\n",
    "#  But also since our data is not computationally expensive the test data does not need to be less than 20% either\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling (50 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The implementation of logistic regression must be written only from the examples given to you by the instructor. No credit will be assigned to teams that copy implementations from another source, regardless of if the code is properly cited. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [20 points] Create a custom, one-versus-all logistic regression classifier using numpy and scipy to optimize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use object oriented conventions identical to scikit-learn. You should start with the template developed by the instructor in the course. You should add the following functionality to the logistic regression classifier:Ability to choose optimization technique when class is instantiated: either steepest descent, stochastic gradient descent, or Newton's method.  Update the gradient calculation to include a customizable regularization term (either using no regularization, L1 regularization, L2 regularization, or both L1 and L2 regularization). Associate a cost with the regularization term, \"C\", that can be adjusted when the class is instantiated.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLR\n",
    "#inherit from base class\n",
    "# from last time, our logistic regression algorithm is given by (including everything we previously had):\n",
    "from sklearn import metrics as mt\n",
    "from scipy.special import expit\n",
    "from numpy.linalg import pinv\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001, optChoice='steepest', reg_choice = \"o\"):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.optChoice = optChoice\n",
    "        self.reg_choice = reg_choice\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "\n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "\n",
    "\n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "\n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "\n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "\n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate\n",
    "\n",
    "    # public:\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "\n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "\n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vector BinaryLogisticRegression\n",
    "class VectorBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # inherit from our previous class to get same functionality\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "\n",
    "    # but overwrite the gradient calculation\n",
    "    def _get_gradient(self,X,y):\n",
    "        if self.optChoice == 'steepest':\n",
    "            ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "            gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "            gradient = gradient.reshape(self.w_.shape)\n",
    "\n",
    "\n",
    "            l_choice = self.reg_choice\n",
    "            if l_choice == \"o\":\n",
    "                gradient += gradient.reshape(self.w_.shape)\n",
    "            elif l_choice == \"l1\":\n",
    "                gradient[1:] += -np.sin(self.w_[1:]) * self.C\n",
    "            elif l_choice == \"l2\":\n",
    "                gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "            elif l_choice == \"both\":\n",
    "                gradient[1:] += (-np.sin(self.w_[1:]) + (-2 * self.w_[1:])) * self.C\n",
    "\n",
    "            return gradient\n",
    "        elif self.optChoice == 'stochastic':\n",
    "            # stochastic gradient calculation\n",
    "            idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "            ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "            gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "            gradient = gradient.reshape(self.w_.shape)\n",
    "\n",
    "\n",
    "            l_choice = self.reg_choice\n",
    "            if l_choice == \"o\":\n",
    "                gradient += gradient.reshape(self.w_.shape)\n",
    "            elif l_choice == \"l1\":\n",
    "                gradient[1:] += -np.sin(self.w_[1:]) * self.C\n",
    "            elif l_choice == \"l2\":\n",
    "                gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "            elif l_choice == \"both\":\n",
    "                gradient[1:] += (-np.sin(self.w_[1:]) + (-2 * self.w_[1:])) * self.C\n",
    "\n",
    "\n",
    "            return gradient\n",
    "        elif self.optChoice == 'newtonHessian':\n",
    "            g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "            hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.C # calculate the hessian\n",
    "            ydiff = y-g # get y difference\n",
    "            gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "            gradient = gradient.reshape(self.w_.shape)\n",
    "\n",
    "            l_choice = self.reg_choice\n",
    "            if l_choice == \"o\":\n",
    "                gradient += gradient.reshape(self.w_.shape)\n",
    "            elif l_choice == \"l1\":\n",
    "                gradient[1:] += -np.sin(self.w_[1:]) * self.C\n",
    "            elif l_choice == \"l2\":\n",
    "                gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "            elif l_choice == \"both\":\n",
    "                gradient[1:] += (-np.sin(self.w_[1:]) + (-2 * self.w_[1:])) * self.C\n",
    "\n",
    "            return pinv(hessian) @ gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "class LogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001, optChoice='steepest', reg_choice=\"o\"):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.optChoice = optChoice\n",
    "        self.reg_choice = reg_choice\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = VectorBinaryLogisticRegression(self.eta,self.iters,self.C,self.optChoice, self.reg_choice)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "\n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "\n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "\n",
    "        return np.hstack(probs) # make into single matrix\n",
    "\n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def getCArray(beginC, endC, stepSize):\n",
    "    cArr = []\n",
    "    for i in np.arange(beginC, endC, stepSize).tolist():\n",
    "        cArr.append(i)\n",
    "    return cArr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snoopReg(beginC, endC, stepSize, X_train, y_train,X_test,y_test, regression):\n",
    "    accuracyArr = []\n",
    "    for i in np.arange(beginC, endC, stepSize).tolist():\n",
    "        #Choose the optimization and the L term\n",
    "        if (regression == \"lr_steep0\"):\n",
    "            lr = LogisticRegression(eta=0.1, C = i)\n",
    "        elif (regression == \"lr_steep1\"):\n",
    "            lr = LogisticRegression(eta=0.1,C = i,reg_choice = \"l1\")\n",
    "        elif (regression == \"lr_steep2\"):\n",
    "            lr = LogisticRegression(eta=0.1, C = i, reg_choice = \"l2\")\n",
    "        elif (regression == \"lr_steepb\"):\n",
    "            lr = LogisticRegression(eta=0.1, C= i, reg_choice = \"both\")\n",
    "        elif (regression == \"lr_scho0\"):\n",
    "            lr = LogisticRegression(eta=0.1,iterations=1500,C = i, optChoice = 'stochastic')\n",
    "        elif (regression == \"lr_scho1\"):\n",
    "            lr = LogisticRegression(eta=0.1,iterations=1500, C = i, optChoice = 'stochastic',reg_choice = \"l1\")\n",
    "        elif (regression == \"lr_scho2\"):\n",
    "            lr = LogisticRegression(eta=0.1,iterations=1500, C = i, optChoice = 'stochastic',reg_choice = \"l2\")\n",
    "        elif (regression == \"lr_schob\"):\n",
    "            lr = LogisticRegression(eta=0.1,iterations=1500, C = i, optChoice = 'stochastic',reg_choice = \"both\")\n",
    "        elif (regression == \"lr_nh0\"):\n",
    "            lr = LogisticRegression(eta=0.1,iterations=1, C = i, optChoice = 'newtonHessian')\n",
    "        elif (regression == \"lr_nh1\"):\n",
    "            lr = LogisticRegression(eta=0.1,iterations=1, C = i, optChoice = 'newtonHessian',reg_choice = \"l1\")\n",
    "        elif (regression == \"lr_nh2\"):\n",
    "            lr = LogisticRegression(eta=0.1,iterations=1, C = i, optChoice = 'newtonHessian',reg_choice = \"l2\")\n",
    "        elif (regression == \"lr_nhb\"):\n",
    "            lr = LogisticRegression(eta=0.1,iterations=1, C = i, optChoice = 'newtonHessian',reg_choice = \"both\")\n",
    "        lr.fit(X_train,y_train)  # train object\n",
    "        y_hat = lr.predict(X_test) # get test set precitions\n",
    "        acc = mt.accuracy_score(y_test,y_hat)\n",
    "        accuracyArr.append(acc)\n",
    "    return accuracyArr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [15 points] Train your classifier to achieve good generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " That is, adjust the optimization technique and the value of the regularization term \"C\" to achieve the best performance on your test set. Visualize the performance of the classifier versus the parameters you investigated. Is your method of selecting parameters justified? That is, do you think there is any \"data snooping\" involved with this method of selecting parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regListName = [\"Steepest-Orig :\", \"Steepest-1 :\", \"Steepest-2 :\", \"Steepest-B :\", \"Stochastic-Orig: \", \"Stochastic-1: \", \"Stochastic-2: \", \"Stochastic-B: \", \"NewtonHessian-Orig: \", \"NewtonHessian-1: \", \"NewtonHessian-2: \", \"NewtonHessian-B: \"]\n",
    "regList = [\"lr_steep0\", \"lr_steep1\", \"lr_steep2\", \"lr_steepb\", \"lr_scho0\", \"lr_scho1\", \"lr_scho2\", \"lr_schob\", \"lr_nh0\", \"lr_nh1\", \"lr_nh2\", \"lr_nhb\"]\n",
    "cList = [0.001,1,0.01]\n",
    "i = 0\n",
    "bestC = []\n",
    "for r in regList:\n",
    "    regArr = snoopReg(beginC = cList[0], endC = cList[1], stepSize = cList[2], X_train = X_train, y_train = y_train, X_test = X_test,y_test = y_test, regression = r)\n",
    "    cArr = getCArray(beginC = cList[0], endC = cList[1], stepSize = cList[2])\n",
    "    print(regListName[i])\n",
    "    print(\"max accuracy: \" , max(regArr))\n",
    "    c_value_index = regArr.index(max(regArr))\n",
    "    bestC.append(cArr[c_value_index])\n",
    "    print(\"c value: \", cArr[c_value_index])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [15 points] Compare the performance of your \"best\" logistic regression optimization procedure to the procedure used in scikit-learn. Visualize the performance differences in terms of training time and classification performance. Discuss the results. \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "lr_sk = SKLogisticRegression() # all params default\n",
    "\n",
    "lr_sk.fit(X,y)\n",
    "yhat = lr_sk.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deployment (10 points total)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which implementation of logistic regression would you advise be used in a deployed machine learning model, your implementation or scikit-learn (or other third party)? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exceptional Work (10 points total)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have free reign to provide additional analyses. One idea: Update the code to use either \"one-versus-all\" or \"one-versus-one\" extensions of binary to multi-class classification. \n",
    "        One idea (required for 7000 level students): Implement an optimization technique for logistic regression using mean square error as your objective function (instead of binary entropy). Your solution should be able to solve the binary logistic regression problem in one gradient update step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
